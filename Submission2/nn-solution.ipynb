{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "zGiS4hdWQPzQ",
        "outputId": "2324c01e-096c-4eb0-c07c-3bd5097452cc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (49) must match the size of tensor b (0) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a2a992314f9b>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_pred_t0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_x_t0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_xxx_t0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a2a992314f9b>\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(u_pred, u_x_pred, u_xxx_pred, u_true, lambda_1, lambda_2)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_x_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_xxx_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mu_x_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mu_xxx_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mmse_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmse_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mu_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (49) must match the size of tensor b (0) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Neural Network Definition\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 50)  # Input is now 2D: (x_interior, time)\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.fc3 = nn.Linear(50, 50)\n",
        "        self.fc4 = nn.Linear(50, 50)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = torch.tanh(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Loss function\n",
        "def loss_function(u_pred, u_x_pred, u_xxx_pred, u_true, lambda_1, lambda_2):\n",
        "    f = u_pred[:, 1:] + lambda_1 * u_pred[:, :-1] * u_x_pred[:, :-1] + lambda_2 * u_xxx_pred[:, :-1]\n",
        "    mse_c = torch.mean(f**2)\n",
        "    mse_s = torch.mean((u_pred[:, 0] - u_true)**2)\n",
        "    mse_b = torch.mean((u_pred[:, -1])**2)\n",
        "    return mse_c + mse_s + mse_b\n",
        "\n",
        "# Compute first and third derivatives\n",
        "def derivatives(u, x):\n",
        "    u_x = autograd.grad(u.sum(), x, create_graph=True)[0]\n",
        "    u_xx = autograd.grad(u_x.sum(), x, create_graph=True)[0]\n",
        "    u_xxx = autograd.grad(u_xx.sum(), x, create_graph=True)[0]\n",
        "    return u_x, u_xxx\n",
        "\n",
        "# Initialize the network and optimizer\n",
        "net = Net()\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
        "lambda_1 = torch.tensor([1.0], requires_grad=True)\n",
        "lambda_2 = torch.tensor([1.0], requires_grad=True)\n",
        "\n",
        "# Data for plotting\n",
        "epochs = 2000\n",
        "loss_history = []\n",
        "lambda_1_history = []\n",
        "lambda_2_history = []\n",
        "\n",
        "# Sampling points\n",
        "x_interior = torch.rand(250, 1, requires_grad=True) * 2 - 1  # Interior points from [-1, 1]\n",
        "input_t0 = torch.cat((x_interior, t_0), dim=1)\n",
        "input_t1 = torch.cat((x_interior, t_1), dim=1)\n",
        "\n",
        "t_0 = torch.full((250, 1), 0.1)\n",
        "t_1 = torch.full((250, 1), 0.9)\n",
        "u_true = 2 / torch.cosh(x_interior)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass for time t_0 and t_1\n",
        "    u_pred_t0 = net(input_t0)\n",
        "    u_pred_t1 = net(input_t1)\n",
        "\n",
        "    # Compute derivatives with respect to x_interior\n",
        "    u_x_t0, u_xxx_t0 = derivatives(u_pred_t0, x_interior)\n",
        "    u_x_t1, u_xxx_t1 = derivatives(u_pred_t1, x_interior)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = loss_function(u_pred_t0, u_x_t0, u_xxx_t0, u_true, lambda_1, lambda_2)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Store loss and lambda values for plotting\n",
        "    loss_history.append(loss.item())\n",
        "    lambda_1_history.append(lambda_1.item())\n",
        "    lambda_2_history.append(lambda_2.item())\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}, λ1: {lambda_1.item()}, λ2: {lambda_2.item()}')\n",
        "\n",
        "        # Plot the prediction at the current epoch\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(x_interior.detach().numpy(), u_pred_t0[:, 0].detach().numpy(), label=f'u_pred at t_0, epoch {epoch}')\n",
        "        plt.plot(x_interior.detach().numpy(), u_true.detach().numpy(), label='u_true at t_0', linestyle='dashed')\n",
        "        plt.title(f'Prediction vs True Solution at Epoch {epoch}')\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('u')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "# Plot the loss over epochs\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(loss_history, label=\"Training Loss\")\n",
        "plt.title(\"Loss over Training Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot lambda_1 and lambda_2 over epochs\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(lambda_1_history, label=\"λ1 over epochs\")\n",
        "plt.plot(lambda_2_history, label=\"λ2 over epochs\")\n",
        "plt.title(\"Parameter λ1 and λ2 Convergence\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3SDdW0hQQRGa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}